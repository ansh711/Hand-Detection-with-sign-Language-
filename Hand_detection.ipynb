{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DEPENDENCIES AND NEEDED MODULES **"
      ],
      "metadata": {
        "id": "xcyA82Sc8nKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DFEIv-lAZM4",
        "outputId": "e5c895fc-34e6-4b0c-8413-0ac7deae8859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.15)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.4)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import uuid\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "B0zUD8MGAvEf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands"
      ],
      "metadata": {
        "id": "gApKM4OzAvIA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('Output Images')"
      ],
      "metadata": {
        "id": "nDa6OnSMFMsa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SIMPLE HAND DETECTION**"
      ],
      "metadata": {
        "id": "oc9-_-1Z8dw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete hand detection by cv2.... if we place our hand infront of our webcam ,, we can see the lines which specifies the hand we are showing by mediapipe\n",
        "from google.colab.patches import cv2_imshow\n",
        "cap = cv2.VideoCapture(0)\n",
        "# check whether the webcam is opened or not\n",
        "if not cap.isOpened():\n",
        "  raise Exception(\"Cannot open webcam\")\n",
        "# hand processing method\n",
        "with mp_hands.Hands(min_detection_confidence=0.8 , min_tracking_confidence=0.5) as hands:\n",
        "  # process the cap\n",
        "  while cap.isOpened():\n",
        "    ret , frame = cap.read()\n",
        "    # check frame is read correctly\n",
        "    if not ret:\n",
        "          print(\"Can't receive frame (Stream end). Exiting......\")\n",
        "          break\n",
        "    # BGR 2 RGB\n",
        "    img = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
        "    # flip the image on horizontal\n",
        "    img = cv2.flip(img,1)\n",
        "    # set the flag as false\n",
        "    img.flags.writeable = False\n",
        "    # detect the hand in the web cam\n",
        "    results = hands.process(img)\n",
        "    # set the flag as true\n",
        "    img.flags.writeable = True\n",
        "    # RGB to BGR\n",
        "    img = cv2.cvtColor(img ,cv2.COLOR_RGB2BGR)\n",
        "    # print the results\n",
        "    print(results)\n",
        "    # render the whole result of the detected hand\n",
        "    if results.multi_hand_landmarks:\n",
        "      for num , hand in enumerate(results.multi_hand_landmarks):\n",
        "        mp_drawing.draw_landmarks(img , hand , mp_hands.HAND_CONNECTIONS,mp_drawing.DrawingSpec(color = (121,22,76), thickness=2 , circle_radius=4),mp_drawing.DrawingSpec(color = (250 , 44 , 250) , thickness = 2 , circle_radius = 2), )\n",
        "    cv2.imwrite(os.path.join('Output Images' , '{}.jpg'.format(uuid.uuid1())) , img)\n",
        "    cv2_imshow('Hand Tracking' , img)\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "6XNzoAfVAvLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e077e313-25b6-4360-d2d1-edceef50c53e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Cannot open webcam",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c9f00d50a832>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check whether the webcam is opened or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot open webcam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# hand processing method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmp_hands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_detection_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmin_tracking_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Cannot open webcam"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **COLLECT THE IMAGES FROM REAL TIME WEBCAM FROM OUR COMPUTER**"
      ],
      "metadata": {
        "id": "kyt5aUDU8JMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "DATA_DIR = './E:/Sign_Language_Detection/Data'\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "# collect images of 36 different classes with 1000 jpg files of each ... so our ultimate dataset will have 3600 jpg imgaes as the training data\n",
        "number_of_classes = 36\n",
        "dataset_size = 100\n",
        "# now capture the real time images\n",
        "cap = cv2.VideoCapture(2)\n",
        "# check if the webcam is opened\n",
        "if not cap.isOpened():\n",
        "  raise Exception(\"Cannot open webcam\")\n",
        "\n",
        "for j in range(number_of_classes):\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
        "        os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
        "# data collection is going on .............\n",
        "    print('Collect the data for class {}'.format(j))\n",
        "\n",
        "    done = False\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        # check frame is read correctly\n",
        "        if not ret:\n",
        "          print(\"Can't receive frame (Stream end). Exiting......\")\n",
        "          break\n",
        "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,\n",
        "                    cv2.LINE_AA)\n",
        "        cv2_imshow(frame)\n",
        "        if cv2.waitKey(25) == ord('q'):\n",
        "            break\n",
        "# after every class of different data is collected counter will increase by 1 and will follow the loops and capture images and make 1000 jpgs of every class\n",
        "    counter = 0\n",
        "    while counter < dataset_size:\n",
        "        ret, frame = cap.read()\n",
        "        # check frame is read correctly\n",
        "        if not ret:\n",
        "          print(\"Can't receive frame (Stream end). Exiting......\")\n",
        "          break\n",
        "        cv2_imshow(frame)\n",
        "        cv2.waitKey(25)\n",
        "        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
        "\n",
        "        counter += 1\n",
        "# data collection done !!!!!!!!!!!!!!! yayyyyyy!!!!\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ymGMapjgAvN4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6a3c2058-8802-4c19-90c8-c01af2de8790"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Cannot open webcam",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6699d334d01e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# check if the webcam is opened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot open webcam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Cannot open webcam"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKING A DATASET OF OUR OWN\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
        "\n",
        "DATA_DIR = './E:/Sign_Language_Detection/Data'\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "for dir_ in os.listdir(DATA_DIR):\n",
        "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
        "        data_aux = []\n",
        "\n",
        "        x_ = []\n",
        "        y_ = []\n",
        "\n",
        "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        results = hands.process(img_rgb)\n",
        "        # MAKING THE LANDMARKS ON OUR BEFORE CREATED JPG FILES [AN ALTERNATIVE IDEA OF USING LABLEIMG]\n",
        "        # Because Python has Mediapipe for all this mannual works ................ hehe\n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                for i in range(len(hand_landmarks.landmark)):\n",
        "                    x = hand_landmarks.landmark[i].x\n",
        "                    y = hand_landmarks.landmark[i].y\n",
        "\n",
        "                    x_.append(x)\n",
        "                    y_.append(y)\n",
        "\n",
        "                for i in range(len(hand_landmarks.landmark)):\n",
        "                    x = hand_landmarks.landmark[i].x\n",
        "                    y = hand_landmarks.landmark[i].y\n",
        "                    data_aux.append(x - min(x_))\n",
        "                    data_aux.append(y - min(y_))\n",
        "\n",
        "            data.append(data_aux)\n",
        "            labels.append(dir_)\n",
        "\n",
        "f = open('data.pickle', 'wb')\n",
        "pickle.dump({'data': data, 'labels': labels}, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "CdK9J67pAvQU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and tune the model using Random Forest Classifier**"
      ],
      "metadata": {
        "id": "7Dlecs2Xa48h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic model formation by using the random forest classifier for the finite node containing graphs which is created on the hands we are showing in cv2\n",
        "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
        "# data and labels are being processed in an np array\n",
        "data = np.asarray(data_dict['data'])\n",
        "labels = np.asarray(data_dict['labels'])\n",
        "# splitting our training and testing set by using train_test_split on all the data we have collected real timely\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
        "# train set = train and tune our classifier using cross validation\n",
        "# test set = Don't ever touch it untill the end or I will kill you\n",
        "# shuffle the datas thus we can't figure out which is train set and which is test set\n",
        "# 3600*0.2 = 720 datas are in test set and remaining 2880 datas are in train set to validate and fine tune our classifier model\n",
        "# number of classes are = 36 ..... This is gonna be very interesting if it can guess rightly or not .... let's see\n",
        "\n",
        "# Random Forest Classifier is being used for this type of classification because of traversing and classifing finite node containe graphs\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "#very fast and robust .... that's why I have chosen this simple ML algorithm in this case\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# prediction of the model\n",
        "y_predict = model.predict(x_test)\n",
        "\n",
        "score = accuracy_score(y_predict, y_test)\n",
        "\n",
        "print('{}% of samples were classified correctly !'.format(score * 100))\n",
        "\n",
        "f = open('model.p', 'wb')\n",
        "pickle.dump({'model': model}, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "V08tI-7hHSCQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "95c7920a-8d2c-4320-c2ec-769df9481bdb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d2317e2e9425>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2649\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2306\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the classifier model using real time sign language conversation**"
      ],
      "metadata": {
        "id": "b3PIWtdLbSbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the web cam and test the classifier model[model.p] using real time images and sign language detection\n",
        "model_dict = pickle.load(open('./model.p', 'rb'))\n",
        "model = model_dict['model']\n",
        "\n",
        "cap = cv2.VideoCapture(2)\n",
        "# check if the webcam is opened\n",
        "if not cap.isOpened():\n",
        "  raise Exception(\"Cannot open webcam\")\n",
        "\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
        "\n",
        "labels_dict = {0: 'A', 1: 'B', 2: 'C' , 3: 'D' , 4: 'E' , 5:'F' , 6:'G' , 7:'H' , 8:'I' , 9:'J' , 10:'K' , 11:'L' , 12:'M' , 13:'N' , 14:'O' , 15:'P', 16:'Q' , 17:'R' , 18:'S' ,\n",
        "               19:'T' , 20:'U' , 21:'V' , 22:'W' , 23:'X' , 24:'Y' , 25:'Z' , 26:'0' , 27:'1' , 28:'2' , 29:'3' , 30:'4' , 31:'5' , 32:'6' , 33:'7' , 34:'8' , 35:'9'}\n",
        "while True:\n",
        "\n",
        "    data_aux = []\n",
        "    x_ = []\n",
        "    y_ = []\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    # check frame is read correctly\n",
        "    if not ret:\n",
        "          print(\"Can't receive frame (Stream end). Exiting......\")\n",
        "          break\n",
        "\n",
        "    H, W, _ = frame.shape\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "# result will be defined the process on the coloured image BGR 2 RGB\n",
        "    results = hands.process(frame_rgb)\n",
        "    if results.multi_hand_landmarks:\n",
        "      # checking and displaying the hand detection with landmarkings\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                frame,  # image to draw\n",
        "                hand_landmarks,  # model output\n",
        "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
        "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                mp_drawing_styles.get_default_hand_connections_style())\n",
        "# putting the landmarks again in the real time video\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            for i in range(len(hand_landmarks.landmark)):\n",
        "                x = hand_landmarks.landmark[i].x\n",
        "                y = hand_landmarks.landmark[i].y\n",
        "\n",
        "                x_.append(x)\n",
        "                y_.append(y)\n",
        "\n",
        "            for i in range(len(hand_landmarks.landmark)):\n",
        "                x = hand_landmarks.landmark[i].x\n",
        "                y = hand_landmarks.landmark[i].y\n",
        "                data_aux.append(x - min(x_))\n",
        "                data_aux.append(y - min(y_))\n",
        "# the eucledian parameter or projection and the dimensions which is making to be traversed [the corners of the rectangle around the hand]\n",
        "        x1 = int(min(x_) * W) - 10\n",
        "        y1 = int(min(y_) * H) - 10\n",
        "\n",
        "        x2 = int(max(x_) * W) - 10\n",
        "        y2 = int(max(y_) * H) - 10\n",
        "\n",
        "        # make the  prediction by making the data in a form of nparray\n",
        "\n",
        "        prediction = model.predict([np.asarray(data_aux)])\n",
        "\n",
        "        predicted_character = labels_dict[int(prediction[0])]\n",
        "# make a rectangle around the hand and make prediction in text\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
        "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
        "                    cv2.LINE_AA)\n",
        "\n",
        "    cv2_imshow('frame', frame)\n",
        "    cv2.waitKey(1)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "DiatwIsMHSEZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e61fbd10-84f9-4d91-fe43-1cf821c04678"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './model.p'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bd55cb436889>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Access the web cam and test the classifier model[model.p] using real time images and sign language detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model.p'"
          ]
        }
      ]
    }
  ]
}